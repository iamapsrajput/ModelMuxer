# Frequently Asked Questions

## Installation and Setup

### Q: What Python version is required?
A: ModelMuxer requires Python 3.11 or higher.

### Q: Which LLM providers are supported?
A: Currently supported providers include OpenAI, Anthropic, and Mistral.

### Q: How do I get API keys for providers?
A: Visit each provider's website to sign up and generate API keys:
- OpenAI: https://platform.openai.com/api-keys
- Anthropic: https://console.anthropic.com/
- Mistral: https://console.mistral.ai/

## Configuration

### Q: How do I configure budget limits?
A: Set budget limits in your environment variables or through the API endpoints. See the configuration guide for details.

### Q: Can I use multiple routing strategies?
A: Yes, you can switch between routing strategies or configure different strategies for different use cases.

## Usage

### Q: How does intelligent routing work?
A: ModelMuxer analyzes your prompts and automatically selects the most appropriate model based on factors like cost, quality requirements, and task complexity.

### Q: Is streaming supported?
A: Yes, ModelMuxer supports streaming responses from all supported providers.

## Troubleshooting

For technical issues, see the [Troubleshooting Guide](troubleshooting.md).

## Getting Help

If you have questions not covered here:
- Check the [documentation](docs/)
- Open an issue on GitHub
- Join our community discussions
